package metoClassifier;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;

import utils.BrownCluster_Str;
import utils.Embeddings_Vec;
import utils.GoogleNgram;
import utils.JWNLUtils;
import utils.Levin;

import libsvm.svm_node;
import model.Dependency;
import model.Document;
import model.Feature;
import model.Global;
import model.NamedEntity;
import model.NominalFeature;
import model.Sentence;
import model.Word;

public class FeatureExtractor implements Serializable {

	/**
	 * 
	 */
	private static final long serialVersionUID = 1L;

	String[] featureHeads = { "parse" }; // default is the parse feature only.

	int fValLen;

	public int getFValLen() {
		return this.fValLen;
	}

	public FeatureExtractor(String... heads) {
		this.featureHeads = heads;
		System.err.println("Feautre Value Length is set to 0, "
				+ "then it's not checked for consistancy with feature length generated by feature extractor.");
	}

	public FeatureExtractor(int fValLen, String... heads) {
		this.featureHeads = heads;
		this.fValLen = fValLen;
	}

	public ArrayList<ArrayList<svm_node>> extract(ArrayList<Document> docs) throws Exception {

		if (docs == null)
			return null;
		if (docs.size() == 0)
			return null;

		ArrayList<ArrayList<svm_node>> features = null;

		for (String featureHead : featureHeads) {
			ArrayList<ArrayList<svm_node>> feature = null;
			if (featureHead.equals("context")) {
				feature = extractContextFeature(docs);
			}
			if (featureHead.equals("determiner")) {
				feature = extractDeterminerFeature(docs);
			}
			if (featureHead.equals("parse")) {
				feature = extractParseFeature(docs);
			}
			if (features == null)
				features = feature;
			else {
				for (int i = 0; i < feature.size(); i++) {
					features.get(i).addAll(feature.get(i));
				}
			}
		}

		return features;

	}

	private ArrayList<ArrayList<svm_node>> extractDeterminerFeature(ArrayList<Document> docs) {

		ArrayList<ArrayList<svm_node>> features = new ArrayList<ArrayList<svm_node>>();

		for (Document doc : docs) {
			List<Sentence> sents = doc.getParagraphs().get(0).getSentences();
			for (Sentence sent : sents) {
				if (sent.getEntities().size() != 0) {
					NamedEntity ne = sent.getNamedEntities().get(0);
					int s = ne.getStart();
					int e = ne.getEnd();
					boolean determiner = false;

					// add -1 word
					if (s >= 2)
						determiner = utils.StringUtils.isDeterminer(sent.getWords().get(s - 2).getWord());
					if (s >= 1)
						determiner = utils.StringUtils.isDeterminer(sent.getWords().get(s - 1).getWord());
					ArrayList<svm_node> nodes = new ArrayList<svm_node>();
					nodes.add(new svm_node());
					nodes.get(0).index = 0;
					nodes.get(0).value = determiner ? 1 : 0;
					features.add(nodes);
					// System.out.println(determiner);
				}
			}
		}
		return features;
	}

	private ArrayList<ArrayList<svm_node>> extractParseFeature(ArrayList<Document> docs) throws Exception {

		ArrayList<ArrayList<svm_node>> features = new ArrayList<ArrayList<svm_node>>();

		NominalFeature frelations = new NominalFeature("Feature: Parse:tRelations");
		NominalFeature ftarget = new NominalFeature("Feature: Parse:target");
		NominalFeature fPOS = new NominalFeature("Feature: Parse:targetPOS");
		int doccount = 0;
		for (Document doc : docs) {
			// doccount++;

			ArrayList<svm_node> feat = new ArrayList<svm_node>();
			String t = "[N/A]", s = "[N/A]", relt = "[N/A]", rels = "[N/A]";
			String tpos = null, spos = null;
			boolean containsNE = false;
			for (Sentence sent : doc.getParagraphs().get(0).getSentences()) {
				if (sent.getEntities().size() == 0)
					continue;
				containsNE = true;
				NamedEntity ne = sent.getNamedEntities().get(0);
				Word head = ne.getLastWord();

				for (Dependency dep : sent.getP()) {
					Word src = dep.getSource();
					Word tgt = dep.getTarget();
					String rel = dep.getRelation();
					if (src.equals(head)) {
						t = tgt.getLemma();
						tpos = tgt.getPOS();
						// t = JWNLUtils.getWordNetSenseHead(tgt);
						relt = rel;
					} else if (tgt.equals(head)) {
						s = src.getLemma();
						spos = src.getPOS();
						// s = JWNLUtils.getWordNetSenseHead(src);
						rels = rel;
					}
					frelations.addFeatureValue(relt);
					frelations.addFeatureValue(rels);
					ftarget.addFeatureValue(t);
					ftarget.addFeatureValue(s);
					fPOS.addFeatureValue(spos);
					fPOS.addFeatureValue(tpos);
				}

			}
			if (containsNE)
				doccount++;
		}
		System.out.println(doccount);

		for (Document doc : docs) {
			ArrayList<svm_node> feat = new ArrayList<svm_node>();
			String t = "[N/A]", s = "[N/A]", relt = "[N/A]", rels = "[N/A]";
			 String nt="[N/A]", ns="[N/A]";
			String tpos=null, spos=null;
			for (Sentence sent : doc.getParagraphs().get(0).getSentences()) {
				if (sent.getEntities().size() == 0)
					continue;
				NamedEntity ne = sent.getNamedEntities().get(0);
				Word head = ne.getLastWord();

				for (Dependency dep : sent.getP()) {
					Word src = dep.getSource();
					Word tgt = dep.getTarget();
					String rel = dep.getRelation();
					if (src.equals(head)) {
						t = tgt.getLemma();
						tpos = tgt.getPOS();
						  nt = JWNLUtils.getWordNetSenseHead(tgt);
						relt = rel;
					} else if (tgt.equals(head)) {
						s = src.getLemma();
						spos = src.getPOS();
						 ns = JWNLUtils.getWordNetSenseHead(src);
						rels = rel;
					}
				}
				// use wordnet OR
				// use one hot coding (Dont forget to change the t and s
				// generation codes
//				feat.addAll(ftarget.getSVMNodeVector(new String[] { t, s }));
//				feat.addAll(fPOS.getSVMNodeVector(new String[]{tpos,spos}));
				
				// wordnet feature
//				feat.addAll(ftarget.getSVMNodeVector(new String[] { nt, ns }));

				
				// use brown cluster
				feat.addAll(BrownCluster_Str.getBrownClusterIdSvmNode(t, 3));
				feat.addAll(BrownCluster_Str.getBrownClusterIdSvmNode(s, 3));

//				// use word embeddings
//				feat.addAll(Embeddings_Vec.getEmbeddingSVMNodes(s));
//				feat.addAll(Embeddings_Vec.getEmbeddingSVMNodes(t));

				// levin category
//				 feat.addAll(Levin.getGetLevinSvmNodes(s));
//				 feat.addAll(Levin.getGetLevinSvmNodes(t));

				// google ngram
//				feat.addAll(GoogleNgram.getGNgramSvmNodeVector(s));
//				feat.addAll(GoogleNgram.getGNgramSvmNodeVector(t));
//				
				feat.addAll(frelations.getSVMNodeVector(new String[] { relt, rels }));
				features.add(feat);
			}

		}
		return features;
	}

	private ArrayList<ArrayList<svm_node>> extractContextFeature(ArrayList<Document> docs) throws Exception {

		ArrayList<ArrayList<svm_node>> features = new ArrayList<ArrayList<svm_node>>();

		NominalFeature fctxt = new NominalFeature("Feature: contextwords");

		for (Document doc : docs) {
			List<Sentence> sents = doc.getParagraphs().get(0).getSentences();
			for (Sentence sent : sents) {
				if (sent.getEntities().size() != 0) {
					NamedEntity ne = sent.getNamedEntities().get(0);
					int s = ne.getStart();
					int e = ne.getEnd();
					String p3 = "[N/A]", p2 = "[N/A]", p1 = "[N/A]", n1 = "[N/A]", n2 = "[N/A]", n3 = "[N/A]";

					if (s >= 3) {
						p3 = sent.getWords().get(s - 3).getLemma();
						p3 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(s-3));
					}
					if (s >= 2) {
						p2 = sent.getWords().get(s - 2).getLemma();
						p2 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(s-2));
					}
					if (s >= 1) {
						p1 = sent.getWords().get(s - 1).getLemma();
						p1 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(s-1));

					}
					if (e < sent.getWords().size() - 1) {
						n1 = sent.getWords().get(e + 1).getLemma();
						n1 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(e + 1));
					}
					if (e < sent.getWords().size() - 2) {
						n2 = sent.getWords().get(e + 2).getLemma();
						n2 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(e + 2));
					}
					if (e < sent.getWords().size() - 3) {
						n3 = sent.getWords().get(e + 3).getLemma();
						n3 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(e + 3));
					}
					fctxt.addFeatureValue(p3);
					fctxt.addFeatureValue(p2);
					fctxt.addFeatureValue(p1);
					fctxt.addFeatureValue(n1);
					fctxt.addFeatureValue(n2);
					fctxt.addFeatureValue(n3);

				}
			}
		}

		for (Document doc : docs) {
			List<Sentence> sents = doc.getParagraphs().get(0).getSentences();
			for (Sentence sent : sents) {
				if (sent.getEntities().size() != 0) {
					NamedEntity ne = sent.getNamedEntities().get(0);
					int s = ne.getStart();
					int e = ne.getEnd();
					String p4 = "[N/A]", p3 = "[N/A]", p2 = "[N/A]", p1 = "[N/A]", n1 = "[N/A]", n2 = "[N/A]", n3 = "[N/A]", n4 = "[N/A]";
					String ap1 =  "[N/A]", an1 =  "[N/A]";
					if (s >= 4)
						p4 = sent.getWords().get(s - 4).getLemma();
					if (s >= 3){
						p3 = sent.getWords().get(s - 3).getLemma();
						p3 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(s-3));
					}
					if (s >= 2){
						p2 = sent.getWords().get(s - 2).getLemma();
						p2 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(s-2));
					}
					if (s >= 1){
						p1 = sent.getWords().get(s - 1).getLemma();
						ap1 =p1;
						p1 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(s-1));
					}
					if (e < sent.getWords().size() - 1) {
						n1 = sent.getWords().get(e + 1).getLemma();
						an1 = n1;
						n1 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(e + 1));

					}
					if (e < sent.getWords().size() - 2) {
						n2 = sent.getWords().get(e + 2).getLemma();
						n2 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(e + 2));
					}
					if (e < sent.getWords().size() - 3) {
						n3 = sent.getWords().get(e + 3).getLemma();
						n3 = JWNLUtils.getWordNetSenseHead(sent.getWords().get(e + 3));
					}
					if (e < sent.getWords().size() - 4) {
						n4 = sent.getWords().get(e + 4).getLemma();
					}

					ArrayList<svm_node> feat = new ArrayList<svm_node>();

					// One Hot Coding
//					 String[] keys = new String[] { p1, n1};
//					 feat.addAll(fctxt.getSVMNodeVector(keys));
					
					
					// word embeddings --- ABANDONED
//					feat.addAll(Embeddings_Vec.getEmbeddingSVMNodes(ap1));
//					feat.addAll(Embeddings_Vec.getEmbeddingSVMNodes(an1));

//					// bc:
//					feat.addAll(BrownCluster_Str.getBrownClusterIdSvmNode(p3, 3));
//					feat.addAll(BrownCluster_Str.getBrownClusterIdSvmNode(p2, 3));
//					feat.addAll(BrownCluster_Str.getBrownClusterIdSvmNode(ap1, 3));
//					feat.addAll(BrownCluster_Str.getBrownClusterIdSvmNode(an1, 3));
//					feat.addAll(BrownCluster_Str.getBrownClusterIdSvmNode(n2, 3));
//					feat.addAll(BrownCluster_Str.getBrownClusterIdSvmNode(n3, 3));

					//levin:
//					feat.addAll(Levin.getGetLevinSvmNodes(p2));
					feat.addAll(Levin.getGetLevinSvmNodes(ap1));
					feat.addAll(Levin.getGetLevinSvmNodes(an1));
//					feat.addAll(Levin.getGetLevinSvmNodes(n2));
					
					// word net 
//					in the context.

					features.add(feat);
				}
			}// end sent loop
		}// end doc loop

		return features;
	}
}
